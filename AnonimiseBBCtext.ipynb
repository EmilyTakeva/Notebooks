{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import wikipediaapi\n",
    "from presidio_anonymizer import OperatorConfig\n",
    "from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "import csv\n",
    "import sys\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_exists(wiki, entry):\n",
    "    page_exists = wiki.page(entry)\n",
    "    if page_exists.exists():\n",
    "        return (f\"{entry}, ({page_exists.fullurl})\", page_exists.summary)\n",
    "    else:\n",
    "        return (f\"PERSON\", None)\n",
    "\n",
    "def find_previous_next(word: set, text: str):\n",
    "    next_pattern = r'{0}\\s*(\\w+)'.format(word)\n",
    "    previous_pattern = r'(\\w+)\\s*{0}'.format(word)\n",
    "\n",
    "    match_next = re.search(next_pattern, text)\n",
    "    match_previous = re.search(previous_pattern, text)\n",
    "\n",
    "    # If a match is found, extract the word\n",
    "    if match_next:\n",
    "        next = match_next.group(1)  # group(1) refers to the first captured group\n",
    "        if next[0].isupper():\n",
    "            word = word + \" \" + next  \n",
    "\n",
    "    if match_previous:\n",
    "        previous = match_previous.group(1)  # group(1) refers to the first captured group\n",
    "        if previous[0].isupper():\n",
    "            word = previous + \" \" + word \n",
    "    #     print(\"The word previous {0} is: {1}\".format(word, previous))\n",
    "    # else:\n",
    "    #     print(\"No match found.\")\n",
    "\n",
    "    return word\n",
    "\n",
    "def anonymise_text(text, analyzer =  AnalyzerEngine(), anonymizer = AnonymizerEngine()):\n",
    "\n",
    "    # step 1: use analyser with spacy to get the names\n",
    "    results = analyzer.analyze(text=text,\n",
    "                            entities=[\"PERSON\"],\n",
    "                            language='en')\n",
    "\n",
    "    # step 2: heuristic of checking the previous and next words for entries that are/are not names\n",
    "    modified_set = []\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for word in [text[e.start:e.end] for e in results]:\n",
    "        word = word.translate(translation_table)\n",
    "        res = find_previous_next(word, text)\n",
    "        if len(res.split(\" \")) == 1:\n",
    "            modified_set.append(\"Not a person\")\n",
    "        else:\n",
    "            modified_set.append(res)\n",
    "\n",
    "    # step 3: call wikipedia api to check if there is an entry. If there is, we assume its a famous person\n",
    "    person_dict = dict()\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('Checking if pages exist', 'en')\n",
    "\n",
    "    for e in [entry for entry in set(modified_set) if entry!=\"Not a person\"]:\n",
    "        person_dict[e] = check_if_exists(wiki_wiki, e)\n",
    "        \n",
    "    # step 4: preparing data for anonymiser\n",
    "    filtered_set = [person_dict[e][0] if e != \"Not a person\" else \"Not a person\" for e in modified_set]\n",
    "    zipped = [i for i in list(zip(results, filtered_set)) if i[1]!=\"Not a person\"]\n",
    "\n",
    "\n",
    "    operators_dict = dict()\n",
    "    results_custom = list()\n",
    "\n",
    "    for e, entry in enumerate(zipped):\n",
    "        operators_dict[f\"ENTRY_{e}\"] = OperatorConfig(\"replace\", {\"new_value\": zipped[e][1]})\n",
    "        results_custom.append(RecognizerResult(entity_type=f\"ENTRY_{e}\", start=entry[0].start, end=entry[0].end, score=entry[0].score))\n",
    "\n",
    "\n",
    "    # step 5: anonymise text\n",
    "    anonymised_result = anonymizer.anonymize(\n",
    "        text=text,\n",
    "        analyzer_results=results_custom,\n",
    "        operators=operators_dict,\n",
    "    )\n",
    "    \n",
    "    return anonymised_result.text, person_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8762/8762 [11:18:13<00:00,  4.64s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "articles = pd.read_csv('sample_articles.csv')\n",
    "\n",
    "for row in tqdm(articles.iterrows(), total=articles.shape[0]):\n",
    "    text = row[1][\"text\"]\n",
    "    anonymised_text, famous_persons = anonymise_text(text)\n",
    "\n",
    "    articles.loc[row[0], \"anonymised_text\"] = anonymised_text\n",
    "    articles.loc[row[0], \"famous_persons\"] = json.dumps(famous_persons)\n",
    "\n",
    "    time.sleep(np.random.randint(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "1       \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "2       \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "3       \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "4       \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "                              ...                        \n",
      "8757    \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "8758    \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "8759    \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "8760    \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "8761    \"{\\\"metadata\\\": {\\\"id\\\": \\\"urn:bbc:ares::asset...\n",
      "Name: JSON_CONTENT, Length: 8762, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(articles[\"JSON_CONTENT\"])\n",
    "articles.to_csv('AnomisedDataSample.csv', index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "def compare_contexts(person, wiki_context, article_context):\n",
    "    template = \"\"\"\n",
    "        [INST]<<SYS>> You are an assistant for question-answering tasks. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Use the following informatuib to determine if the {person} is the same in both contexts:<</SYS>> \n",
    "\n",
    "        Context 1: {wiki_context} \n",
    "        Context 2: {article_context} \n",
    "\n",
    "        Answer: [/INST]\n",
    "        \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "            input_variables=[\"person\", \"wiki_context\", \"article_context\"],\n",
    "            template=template,\n",
    "    )\n",
    "\n",
    "    llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    temperature=0.2,\n",
    "    num_ctx=2048 * 4,\n",
    "    repeat_last_n=-1,\n",
    "    top_p=0.5,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "    prompt_and_model = prompt_template | llm\n",
    "    output = prompt_and_model.invoke({\"person\": person, \"wiki_context\": wiki_context, \"article_context\": article_context})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Hans Rosling, (https://en.wikipedia.org/wiki/Hans_Rosling)', 'Hans Rosling (Swedish pronunciation: [ˈhɑːns ˈrûːslɪŋ]; 27 July 1948 – 7 February 2017) was a Swedish physician, academic and public speaker. He was a professor of international health at Karolinska Institute and was the co-founder and chairman of the Gapminder Foundation, which developed the Trendalyzer software system. He held presentations around the world, including several TED Talks in which he promoted the use of data (and data visualization) to explore development issues. His posthumously published book Factfulness, coauthored with his daughter-in-law Anna Rosling Rönnlund and son Ola Rosling, became an international bestseller.'),)\n"
     ]
    }
   ],
   "source": [
    "compare_contexts(\"Hans Rosling\",\n",
    "                 [famous_persons[i] for i in famous_persons.keys() if famous_persons[i][0] !=\"PERSON\"][0][1],\n",
    "                 text)\n",
    "print(tuple(famous_persons[i] for i in famous_persons.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       {\"Queen Elizabeth II\": [\"Queen Elizabeth II, (...\n",
      "1       {\"Max Hill\": [\"Max Hill, (https://en.wikipedia...\n",
      "2       {\"Kyle Binks\": [\"PERSON\", null], \"Newton Aycli...\n",
      "3       {\"Clare Balding\": [\"Clare Balding, (https://en...\n",
      "4       {\"Ginny Murphy\": [\"PERSON\", null], \"Steve Atki...\n",
      "                              ...                        \n",
      "8757                                                   {}\n",
      "8758    {\"Afzal Kohistani\": [\"Afzal Kohistani, (https:...\n",
      "8759    {\"President Volodymyr Zelensky\": [\"PERSON\", nu...\n",
      "8760    {\"Minister Anil Vij\": [\"PERSON\", null], \"the L...\n",
      "8761    {\"Hans Rosling\": [\"Hans Rosling, (https://en.w...\n",
      "Name: famous_persons, Length: 8762, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(articles[\"famous_persons\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"famous_persons\"].to_csv(\"FamousAndNonFamousPeople.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[articles[\"famous_persons\"].str.contains(\"PERSON\")][\"famous_persons\"].to_csv(\"NotFamous.csv\")\n",
    "articles[~articles[\"famous_persons\"].str.contains(\"PERSON\")][\"famous_persons\"].to_csv(\"Famous.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[articles[\"famous_persons\"].str.contains(\"PERSON\")].to_csv(\"ALLNotFamous.csv\")\n",
    "articles[~articles[\"famous_persons\"].str.contains(\"PERSON\")].to_csv(\"ALLFamous.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
